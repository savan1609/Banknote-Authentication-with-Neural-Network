{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6de055",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3baddb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import  pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7fa2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "note = pd.read_csv('BankNote_Authentication.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7659d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(note.drop(columns=['class'],axis=1))\n",
    "Y = np.array(note['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cde76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fde8d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2957fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "X_test = X_test.T\n",
    "y_test = y_test.reshape(1, y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca6cbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1097)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4796706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is:  = 4\n",
      "The size of the hidden layer is:  = 4\n",
      "The size of the output layer is:  = 1\n"
     ]
    }
   ],
   "source": [
    "def define_structure(X, Y):\n",
    "    input_unit = X.shape[0] # size of input layer\n",
    "    hidden_unit = 4 #hidden layer of size 4\n",
    "    output_unit = Y.shape[0] # size of output layer\n",
    "    return (input_unit, hidden_unit, output_unit)\n",
    "(input_unit, hidden_unit, output_unit) = define_structure(X_train, y_train)\n",
    "print(\"The size of the input layer is:  = \" + str(input_unit))\n",
    "print(\"The size of the hidden layer is:  = \" + str(hidden_unit))\n",
    "print(\"The size of the output layer is:  = \" + str(output_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb1d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_initialization(input_unit, hidden_unit, output_unit):\n",
    "    np.random.seed(2) \n",
    "    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n",
    "    b1 = np.zeros((hidden_unit, 1))\n",
    "    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n",
    "    b2 = np.zeros((output_unit, 1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cebfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e10a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(A2, Y, parameters):\n",
    "    # number of training example\n",
    "    m = Y.shape[1] \n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = float(np.squeeze(cost))\n",
    "                                    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09147f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    #number of training example\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "   \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe00726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, grads, learning_rate = 0.01):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "   \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "179ac08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692969\n",
      "Cost after iteration 5: 0.692721\n",
      "Cost after iteration 10: 0.692469\n",
      "Cost after iteration 15: 0.692210\n",
      "Cost after iteration 20: 0.691940\n",
      "Cost after iteration 25: 0.691655\n",
      "Cost after iteration 30: 0.691352\n",
      "Cost after iteration 35: 0.691027\n",
      "Cost after iteration 40: 0.690674\n",
      "Cost after iteration 45: 0.690289\n",
      "Cost after iteration 50: 0.689867\n",
      "Cost after iteration 55: 0.689401\n",
      "Cost after iteration 60: 0.688885\n",
      "Cost after iteration 65: 0.688314\n",
      "Cost after iteration 70: 0.687681\n",
      "Cost after iteration 75: 0.686978\n",
      "Cost after iteration 80: 0.686199\n",
      "Cost after iteration 85: 0.685337\n",
      "Cost after iteration 90: 0.684386\n",
      "Cost after iteration 95: 0.683339\n",
      "Cost after iteration 100: 0.682190\n",
      "Cost after iteration 105: 0.680933\n",
      "Cost after iteration 110: 0.679561\n",
      "Cost after iteration 115: 0.678070\n",
      "Cost after iteration 120: 0.676453\n",
      "Cost after iteration 125: 0.674706\n",
      "Cost after iteration 130: 0.672821\n",
      "Cost after iteration 135: 0.670795\n",
      "Cost after iteration 140: 0.668620\n",
      "Cost after iteration 145: 0.666292\n",
      "Cost after iteration 150: 0.663804\n",
      "Cost after iteration 155: 0.661151\n",
      "Cost after iteration 160: 0.658327\n",
      "Cost after iteration 165: 0.655326\n",
      "Cost after iteration 170: 0.652145\n",
      "Cost after iteration 175: 0.648778\n",
      "Cost after iteration 180: 0.645222\n",
      "Cost after iteration 185: 0.641474\n",
      "Cost after iteration 190: 0.637533\n",
      "Cost after iteration 195: 0.633399\n",
      "Cost after iteration 200: 0.629071\n",
      "Cost after iteration 205: 0.624551\n",
      "Cost after iteration 210: 0.619843\n",
      "Cost after iteration 215: 0.614950\n",
      "Cost after iteration 220: 0.609879\n",
      "Cost after iteration 225: 0.604635\n",
      "Cost after iteration 230: 0.599224\n",
      "Cost after iteration 235: 0.593656\n",
      "Cost after iteration 240: 0.587939\n",
      "Cost after iteration 245: 0.582080\n",
      "Cost after iteration 250: 0.576089\n",
      "Cost after iteration 255: 0.569975\n",
      "Cost after iteration 260: 0.563747\n",
      "Cost after iteration 265: 0.557415\n",
      "Cost after iteration 270: 0.550988\n",
      "Cost after iteration 275: 0.544476\n",
      "Cost after iteration 280: 0.537888\n",
      "Cost after iteration 285: 0.531234\n",
      "Cost after iteration 290: 0.524525\n",
      "Cost after iteration 295: 0.517769\n",
      "Cost after iteration 300: 0.510978\n",
      "Cost after iteration 305: 0.504162\n",
      "Cost after iteration 310: 0.497331\n",
      "Cost after iteration 315: 0.490495\n",
      "Cost after iteration 320: 0.483664\n",
      "Cost after iteration 325: 0.476848\n",
      "Cost after iteration 330: 0.470057\n",
      "Cost after iteration 335: 0.463300\n",
      "Cost after iteration 340: 0.456586\n",
      "Cost after iteration 345: 0.449922\n",
      "Cost after iteration 350: 0.443316\n",
      "Cost after iteration 355: 0.436775\n",
      "Cost after iteration 360: 0.430305\n",
      "Cost after iteration 365: 0.423912\n",
      "Cost after iteration 370: 0.417601\n",
      "Cost after iteration 375: 0.411376\n",
      "Cost after iteration 380: 0.405241\n",
      "Cost after iteration 385: 0.399199\n",
      "Cost after iteration 390: 0.393253\n",
      "Cost after iteration 395: 0.387404\n",
      "Cost after iteration 400: 0.381655\n",
      "Cost after iteration 405: 0.376006\n",
      "Cost after iteration 410: 0.370459\n",
      "Cost after iteration 415: 0.365015\n",
      "Cost after iteration 420: 0.359672\n",
      "Cost after iteration 425: 0.354431\n",
      "Cost after iteration 430: 0.349292\n",
      "Cost after iteration 435: 0.344254\n",
      "Cost after iteration 440: 0.339316\n",
      "Cost after iteration 445: 0.334477\n",
      "Cost after iteration 450: 0.329737\n",
      "Cost after iteration 455: 0.325094\n",
      "Cost after iteration 460: 0.320546\n",
      "Cost after iteration 465: 0.316093\n",
      "Cost after iteration 470: 0.311732\n",
      "Cost after iteration 475: 0.307462\n",
      "Cost after iteration 480: 0.303282\n",
      "Cost after iteration 485: 0.299190\n",
      "Cost after iteration 490: 0.295184\n",
      "Cost after iteration 495: 0.291262\n",
      "Cost after iteration 500: 0.287422\n",
      "Cost after iteration 505: 0.283664\n",
      "Cost after iteration 510: 0.279984\n",
      "Cost after iteration 515: 0.276382\n",
      "Cost after iteration 520: 0.272856\n",
      "Cost after iteration 525: 0.269403\n",
      "Cost after iteration 530: 0.266023\n",
      "Cost after iteration 535: 0.262713\n",
      "Cost after iteration 540: 0.259472\n",
      "Cost after iteration 545: 0.256298\n",
      "Cost after iteration 550: 0.253190\n",
      "Cost after iteration 555: 0.250145\n",
      "Cost after iteration 560: 0.247163\n",
      "Cost after iteration 565: 0.244242\n",
      "Cost after iteration 570: 0.241380\n",
      "Cost after iteration 575: 0.238575\n",
      "Cost after iteration 580: 0.235828\n",
      "Cost after iteration 585: 0.233135\n",
      "Cost after iteration 590: 0.230495\n",
      "Cost after iteration 595: 0.227908\n",
      "Cost after iteration 600: 0.225372\n",
      "Cost after iteration 605: 0.222886\n",
      "Cost after iteration 610: 0.220448\n",
      "Cost after iteration 615: 0.218057\n",
      "Cost after iteration 620: 0.215712\n",
      "Cost after iteration 625: 0.213412\n",
      "Cost after iteration 630: 0.211156\n",
      "Cost after iteration 635: 0.208942\n",
      "Cost after iteration 640: 0.206770\n",
      "Cost after iteration 645: 0.204638\n",
      "Cost after iteration 650: 0.202546\n",
      "Cost after iteration 655: 0.200492\n",
      "Cost after iteration 660: 0.198476\n",
      "Cost after iteration 665: 0.196496\n",
      "Cost after iteration 670: 0.194553\n",
      "Cost after iteration 675: 0.192644\n",
      "Cost after iteration 680: 0.190769\n",
      "Cost after iteration 685: 0.188927\n",
      "Cost after iteration 690: 0.187118\n",
      "Cost after iteration 695: 0.185340\n",
      "Cost after iteration 700: 0.183593\n",
      "Cost after iteration 705: 0.181876\n",
      "Cost after iteration 710: 0.180189\n",
      "Cost after iteration 715: 0.178530\n",
      "Cost after iteration 720: 0.176900\n",
      "Cost after iteration 725: 0.175296\n",
      "Cost after iteration 730: 0.173720\n",
      "Cost after iteration 735: 0.172169\n",
      "Cost after iteration 740: 0.170645\n",
      "Cost after iteration 745: 0.169145\n",
      "Cost after iteration 750: 0.167669\n",
      "Cost after iteration 755: 0.166218\n",
      "Cost after iteration 760: 0.164789\n",
      "Cost after iteration 765: 0.163384\n",
      "Cost after iteration 770: 0.162000\n",
      "Cost after iteration 775: 0.160639\n",
      "Cost after iteration 780: 0.159298\n",
      "Cost after iteration 785: 0.157979\n",
      "Cost after iteration 790: 0.156680\n",
      "Cost after iteration 795: 0.155401\n",
      "Cost after iteration 800: 0.154141\n",
      "Cost after iteration 805: 0.152901\n",
      "Cost after iteration 810: 0.151679\n",
      "Cost after iteration 815: 0.150476\n",
      "Cost after iteration 820: 0.149290\n",
      "Cost after iteration 825: 0.148123\n",
      "Cost after iteration 830: 0.146972\n",
      "Cost after iteration 835: 0.145838\n",
      "Cost after iteration 840: 0.144721\n",
      "Cost after iteration 845: 0.143620\n",
      "Cost after iteration 850: 0.142534\n",
      "Cost after iteration 855: 0.141465\n",
      "Cost after iteration 860: 0.140410\n",
      "Cost after iteration 865: 0.139371\n",
      "Cost after iteration 870: 0.138346\n",
      "Cost after iteration 875: 0.137335\n",
      "Cost after iteration 880: 0.136338\n",
      "Cost after iteration 885: 0.135356\n",
      "Cost after iteration 890: 0.134387\n",
      "Cost after iteration 895: 0.133431\n",
      "Cost after iteration 900: 0.132488\n",
      "Cost after iteration 905: 0.131558\n",
      "Cost after iteration 910: 0.130640\n",
      "Cost after iteration 915: 0.129735\n",
      "Cost after iteration 920: 0.128842\n",
      "Cost after iteration 925: 0.127961\n",
      "Cost after iteration 930: 0.127091\n",
      "Cost after iteration 935: 0.126233\n",
      "Cost after iteration 940: 0.125386\n",
      "Cost after iteration 945: 0.124550\n",
      "Cost after iteration 950: 0.123725\n",
      "Cost after iteration 955: 0.122910\n",
      "Cost after iteration 960: 0.122106\n",
      "Cost after iteration 965: 0.121312\n",
      "Cost after iteration 970: 0.120529\n",
      "Cost after iteration 975: 0.119755\n",
      "Cost after iteration 980: 0.118991\n",
      "Cost after iteration 985: 0.118237\n",
      "Cost after iteration 990: 0.117492\n",
      "Cost after iteration 995: 0.116756\n"
     ]
    }
   ],
   "source": [
    "def neural_network_model(X, Y, hidden_unit, num_iterations = 1000):\n",
    "    np.random.seed(3)\n",
    "    input_unit = define_structure(X, Y)[0]\n",
    "    output_unit = define_structure(X, Y)[2]\n",
    "    \n",
    "    parameters = parameters_initialization(input_unit, hidden_unit, output_unit)\n",
    "   \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = cross_entropy_cost(A2, Y, parameters)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = gradient_descent(parameters, grads)\n",
    "        if i % 5 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    return parameters\n",
    "parameters = neural_network_model(X_train, y_train, 4, num_iterations=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813500b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(parameters, X):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e8c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 97%\n",
      "Accuracy Test: 97%\n"
     ]
    }
   ],
   "source": [
    "predictions = prediction(parameters, X_train)\n",
    "print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\n",
    "predictions = prediction(parameters, X_test)\n",
    "print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06749183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
